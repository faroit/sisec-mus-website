[{"code": "https://github.com/MTG/DeepConvSep", "is_supervised": true, "description": "We introduce a low-latency monaural source separation framework using a Convolutional Neural Network (CNN). We use a CNN to estimate time-frequency soft masks which are applied for source separation. We evaluate the performance of the neural network on a database comprising of musical mixtures of three instruments: voice, drums, bass as well as other instruments which vary from song to song. The proposed architecture is compared to a Multilayer Perceptron (MLP), achieving on-par results and a significant improvement in processing time. The algorithm was submitted to source separation evaluation campaigns to test efficiency, and achieved competitive results.\n", "uses_augmentation": false, "method_name": "Chandna", "affiliation": "Music Technology Group, Universitat Pompeu Fabra Barcelona", "short": "CHA", "references": ["P. Chandna, M. Miron, J. Janer, and E. Gomez, \u201cMonoaural audio source separation using deep convolutional neural networks,\u201d,  13th International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA 2017)."], "authors": "Pritish Chandna, Jordi Janer, Marius Miron", "email": "marius.miron@upf.edu"}, {"short": "DUR", "is_supervised": false, "description": "", "uses_augmentation": false, "method_name": "Durrieu", "affiliation": "EPFL, T\u00e9l\u00e9com ParisTech", "code": "https://github.com/wslihgt/pyfasst", "references": ["J.-L. Durrieu, B. David, and G. Richard, \u201cA musically motivated mid-level representation for pitch estimation and musical audio source separation,\u201d IEEE Journal on Selected Topics on Signal Processing, vol. 5, no. 6, pp. 1180\u20131191, Oct. 2011."], "authors": "Jean-Louis Durrieu, Bertrand David, Ga\u00ebl Richard", "email": "jean-louis.durrieu@epfl.ch"}, {"code": null, "is_supervised": true, "description": "Deep neural network (DNN) based method to generate binary masks that are used to separate the sources\n", "uses_augmentation": true, "method_name": "Grais2", "affiliation": "University of Surrey, United Kingdom", "short": "GRA2", "references": ["E. Grais, G. Roma, A.J. Simpson, M. Plumbley, \u201cSingle-Channel Audio Source Separation Using Deep Neural Network Ensembles. \u201d Proc. AES 140, 2016, May."], "authors": "Emad M. Grais, Gerard Roma, Andrew J. R. Simpson, Mark D. Plumbley", "email": "grais@surrey.ac.uk"}, {"code": null, "is_supervised": true, "description": "Deep neural network (DNN) based method with L2 and L1 sparsity constraints on the output of the DNN. We are submitting the initial results that we have got so far for this method. We still testing this method but since the deadline for the submission is today we are submitting what we have got so far.\n", "uses_augmentation": true, "method_name": "Grais3", "affiliation": "University of Surrey, United Kingdom", "short": "GRA3", "references": ["E. Grais, G. Roma, A.J. Simpson, M. Plumbley, \u201cSingle-Channel Audio Source Separation Using Deep Neural Network Ensembles. \u201d Proc. AES 140, 2016, May."], "authors": "Emad M. Grais, Gerard Roma, Andrew J. R. Simpson, Mark D. Plumbley", "email": "grais@surrey.ac.uk"}, {"code": "https://github.com/posenhuang/singingvoiceseparationrpca", "is_supervised": false, "description": "", "uses_augmentation": false, "method_name": "Huang", "affiliation": "University of Illinois at Urbana-Champaign, USA", "short": "HUA", "references": ["P. Huang, S. Chen, P. Smaragdis, and M. Hasegawa-Johnson, \u201cSinging-voice separation from monaural recordings using robust principal component analysis,\u201d in Proc. ICASSP, Mar. 2012, pp. 57\u201360."], "authors": "Po-Sen Huang, Scott Deeann Chen, Paris Smaragdis, Mark Hasegawa-Johnson", "email": "huang146@illinois.edu"}, {"code": null, "is_supervised": true, "description": "Ideal Binary Mask", "uses_augmentation": false, "method_name": "Ideal_Binary_Mask", "affiliation": ".", "short": "IBM", "references": [null], "authors": "Ideal Binary Mask", "email": "."}, {"code": null, "is_supervised": true, "description": "apply RPCA with weighted l1 norm, whose weight parameter (lambda) between nuclear norm and l1 norm is set differently for each matrix component. we choose the value of lambda for each frequency bin f to be var(A(f))/var(V(f)), where A and V are the spectrogram of accompaniment and singing voice, respectively.", "uses_augmentation": false, "method_name": "RPCA_with_weighted_l1_norm", "affiliation": "MARG, Seoul National University", "short": "JEO1", "references": ["I.-Y. Jeong and K. Lee, Singing voice separation using RPCA with weighted l1-norm"], "authors": "Il-Young Jeong and Kyogu Lee", "email": "finejuly@snu.ac.kr"}, {"code": null, "is_supervised": true, "description": "apply RPCA with weighted l1 norm (wRPCA), whose weight parameter (lambda) between nuclear norm and l1 norm is set differently for each matrix component. We choose the value of lambda for each frequency bin f to be var(A(f))/var(V(f)), where A and V are the spectrogram of accompaniment and singing voice, respectively. A two-stage approach is applied. At the first stage, singing voice and accompaniments are separated using wRPCA, and vocal activity is detected from the separated singing voice. After that, those are separated again using wRPCA with voice activity information.", "uses_augmentation": false, "method_name": "RPCA_with_weighted_l1_norm_and_vocal_activity_detection", "affiliation": "MARG, Seoul National University", "short": "JEO2", "references": ["I.-Y. Jeong and K. Lee, Singing voice separation using RPCA with weighted l1-norm"], "authors": "Il-Young Jeong and Kyogu Lee", "email": "finejuly@snu.ac.kr"}, {"code": null, "is_supervised": false, "description": "", "uses_augmentation": false, "method_name": "Liutkus1", "affiliation": "Inria, Villers-lès-Nancy, France", "short": "KAM1", "references": ["A. Liutkus, D. FitzGerald, Z. Rafii, and L. Daudet, \u201cScalable audio separation with light kernel additive modelling,\u201d in Proc. ICASSP, Apr. 2015, pp. 76\u201380"], "authors": "Antoine Liutkus, Derry FitzGerald, Zafar Rafii", "email": "antoine.liutkus@inria.fr"}, {"code": null, "is_supervised": false, "description": "", "uses_augmentation": false, "method_name": "Liutkus2", "affiliation": "Inria, Villers-lès-Nancy, France", "short": "KAM2", "references": ["A. Liutkus, D. FitzGerald, Z. Rafii, and L. Daudet, \u201cScalable audio separation with light kernel additive modelling,\u201d in Proc. ICASSP, Apr. 2015, pp. 76\u201380"], "authors": "Antoine Liutkus, Derry FitzGerald, Zafar Rafii", "email": "antoine.liutkus@inria.fr"}, {"short": "KON", "is_supervised": true, "description": "Uses spectrogram, window_size=1024, 11 frames as a segment. Apply RNN on the 11*513 spectrogram. Structure: (11*513) -> 500 rnn -> 500 rnn -> (500 dense left & 500 dense right). Objective: ||mask*y_pred - y_gt|| Optimizer: Adam, lr=1e-3, epoch=200\n", "uses_augmentation": false, "method_name": "RNN_QK", "affiliation": "CVSSP, University of Surrey", "code": null, "references": ["Huang, Po Sen, et al. Joint optimization of masks and deep recurrent neural networks for monaural source separation."], "authors": "Qiuqiang Kong", "email": "q.kong@surrey.ac.uk"}, {"code": null, "is_supervised": true, "description": "In this implementation of DNN-based multichannel source separation, the source spectra are estimated once using a DNN and used to derive a multichannel filter through an iterative expectation-maximization (EM) algorithm with simplified-weighted spatial parameter updates.\n", "uses_augmentation": false, "method_name": "Multichannel_separation_with_single_DNN_and_simplified-weighted_EM_updates", "affiliation": "Inria", "short": "NUG1", "references": ["A. A. Nugraha, A. Liutkus, and E. Vincent, \"Multichannel music separation with deep neural networks,\" in Proc. 24th European Signal Processing Conference, Budapest, Hungary, Aug. 2016.", "A. A. Nugraha, A. Liutkus and E. Vincent, \"Multichannel audio source separation with deep neural networks,\" in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 9, pp. 1652-1664, Sept. 2016."], "authors": "Aditya Arie Nugraha, Antoine Liutkus, Emmanuel Vincent", "email": "aditya.nugraha@inria.fr"}, {"code": null, "is_supervised": true, "description": "In this implementation of DNN-based multichannel source separation, the source spectra are estimated once using a DNN and used to derive a multichannel filter through an iterative expectation-maximization (EM) algorithm with weighted spatial parameter updates.\n", "uses_augmentation": false, "method_name": "Multichannel_separation_with_single_DNN_and_weighted_EM_updates", "affiliation": "Inria", "short": "NUG2", "references": ["A. A. Nugraha, A. Liutkus, and E. Vincent, \"Multichannel music separation with deep neural networks,\" in Proc. 24th European Signal Processing Conference, Budapest, Hungary, Aug. 2016.", "A. A. Nugraha, A. Liutkus and E. Vincent, \"Multichannel audio source separation with deep neural networks,\" in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 9, pp. 1652-1664, Sept. 2016."], "authors": "Aditya Arie Nugraha, Antoine Liutkus, Emmanuel Vincent", "email": "aditya.nugraha@inria.fr"}, {"code": null, "is_supervised": true, "description": "In this implementation of DNN-based multichannel source separation, the source spectra are estimated using two DNNs and used to derive a multichannel filter through an iterative expectation-maximization (EM) algorithm with simplified-weighted spatial parameter updates. The use of multiple DNNs is intended to improve the spectra over the iterations.\n", "uses_augmentation": false, "method_name": "Multichannel_separation_with_two_DNNs_and_simplified-weighted_EM_updates", "affiliation": "Inria", "short": "NUG3", "references": ["A. A. Nugraha, A. Liutkus, and E. Vincent, \"Multichannel music separation with deep neural networks,\" in Proc. 24th European Signal Processing Conference, Budapest, Hungary, Aug. 2016.", "A. A. Nugraha, A. Liutkus and E. Vincent, \"Multichannel audio source separation with deep neural networks,\" in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 9, pp. 1652-1664, Sept. 2016."], "authors": "Aditya Arie Nugraha, Antoine Liutkus, Emmanuel Vincent", "email": "aditya.nugraha@inria.fr"}, {"code": null, "is_supervised": true, "description": "In this implementation of DNN-based multichannel source separation, the source spectra are estimated using two DNNs and used to derive a multichannel filter through an iterative expectation-maximization (EM) algorithm with weighted spatial parameter updates. The use of multiple DNNs is intended to improve the spectra over the iterations.\n", "uses_augmentation": false, "method_name": "Multichannel_separation_with_two_DNNs_and_weighted_EM_updates", "affiliation": "Inria", "short": "NUG4", "references": ["A. A. Nugraha, A. Liutkus, and E. Vincent, \"Multichannel music separation with deep neural networks,\" in Proc. 24th European Signal Processing Conference, Budapest, Hungary, Aug. 2016.", "A. A. Nugraha, A. Liutkus and E. Vincent, \"Multichannel audio source separation with deep neural networks,\" in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 9, pp. 1652-1664, Sept. 2016."], "authors": "Aditya Arie Nugraha, Antoine Liutkus, Emmanuel Vincent", "email": "aditya.nugraha@inria.fr"}, {"short": "OZE", "is_supervised": false, "description": "", "uses_augmentation": false, "method_name": "Ozerov", "affiliation": "INRIA, Rennes, France", "code": "http://bass-db.gforge.inria.fr/fasst/", "references": ["A. Ozerov, E. Vincent, and F. Bimbot, \u201cA general flexible framework for the handling of prior information in audio source separation,\u201d IEEE Trans. ASLP, vol. 20, no. 4, pp. 1118\u20131133, Oct. 2012."], "authors": "Alexey Ozerov, Emmanuel Vincent, Fr\u00e9d\u00e9ric Bimbot", "email": "Alexey.Ozerov@technicolor.com"}, {"code": "http://zafarrafii.com/repet.html", "is_supervised": false, "description": "", "uses_augmentation": false, "method_name": "Rafii1", "affiliation": "Gracenote", "short": "RAF1", "references": [{"Z. Rafii and B. Pardo, \u201cREpeating Pattern Extraction Technique (REPET)": "A simple method for music/voice separation,\u201d IEEE Trans. ASLP, vol. 21, no. 1, pp. 71\u201382, January 2013."}, "A. Liutkus, Z. Rafii, R. Badeau, B. Pardo, and G. Richard, \u201cAdaptive filtering for music/voice separation exploiting the repeating musical structure,\u201d in Proc. ICASSP, Mar. 2012, pp. 53\u201356.", "Z. Rafii and B. Pardo, \u201cMusic/voice separation using the similarity matrix,\u201d in Proc. ISMIR, Oct. 2012, pp. 583\u2013588."], "authors": "Zafar Rafii", "email": "zrafii@gracenote.com"}, {"code": "http://zafarrafii.com/repet.html", "is_supervised": false, "description": "", "uses_augmentation": false, "method_name": "Rafii2", "affiliation": "Gracenote", "short": "RAF2", "references": [{"Z. Rafii and B. Pardo, \u201cREpeating Pattern Extraction Technique (REPET)": "A simple method for music/voice separation,\u201d IEEE Trans. ASLP, vol. 21, no. 1, pp. 71\u201382, January 2013."}, "A. Liutkus, Z. Rafii, R. Badeau, B. Pardo, and G. Richard, \u201cAdaptive filtering for music/voice separation exploiting the repeating musical structure,\u201d in Proc. ICASSP, Mar. 2012, pp. 53\u201356.", "Z. Rafii and B. Pardo, \u201cMusic/voice separation using the similarity matrix,\u201d in Proc. ISMIR, Oct. 2012, pp. 583\u2013588."], "authors": "Zafar Rafii", "email": "zrafii@gracenote.com"}, {"code": "http://zafarrafii.com/repet.html", "is_supervised": false, "description": "", "uses_augmentation": false, "method_name": "Rafii3", "affiliation": "Gracenote", "short": "RAF3", "references": [{"Z. Rafii and B. Pardo, \u201cREpeating Pattern Extraction Technique (REPET)": "A simple method for music/voice separation,\u201d IEEE Trans. ASLP, vol. 21, no. 1, pp. 71\u201382, January 2013."}, "A. Liutkus, Z. Rafii, R. Badeau, B. Pardo, and G. Richard, \u201cAdaptive filtering for music/voice separation exploiting the repeating musical structure,\u201d in Proc. ICASSP, Mar. 2012, pp. 53\u201356.", "Z. Rafii and B. Pardo, \u201cMusic/voice separation using the similarity matrix,\u201d in Proc. ISMIR, Oct. 2012, pp. 583\u2013588."], "authors": "Zafar Rafii", "email": "zrafii@gracenote.com"}, {"short": "STO1", "is_supervised": true, "description": "This approach is based on a Deep neural network (DNN) feed-forward architecture using patched overlapped STFT (See GFT) frames on input and output. In contrast to methods like UHL1, our proposed methd uses temporal context for both, input and output, which leads to improvement in temporal continuity.\n", "uses_augmentation": false, "method_name": "Stoeter1", "affiliation": "International Audio Laboratories Erlangen", "code": "https://github.com/aliutkus/commonfate", "references": ["F.-R. St\u00f6ter, A. Liutkus, R. Badeau, B. Edler, and P. Magron, \u201cCommon Fate Model for Unison source Separation,\u201d in Proc. ICASSP, 2016."], "authors": "Fabian-Robert St\u00f6ter, Bernd Edler", "email": "fabian-robert.stoeter@audiolabs-erlangen.de"}, {"short": "STO2", "is_supervised": true, "description": "Deep neural network (DNN) based method using Common Fate Transform representation (CFT) on network input and output. The CFT representation includes phase information as well. In contrast to methods like UHL1, our proposed methd uses temporal context for both, input and output, which leads to improvement in temporal continuity. \n", "uses_augmentation": false, "method_name": "Stoeter2", "affiliation": "International Audio Laboratories Erlangen", "code": "https://github.com/aliutkus/commonfate", "references": ["F.-R. St\u00f6ter, A. Liutkus, R. Badeau, B. Edler, and P. Magron, \u201cCommon Fate Model for Unison source Separation,\u201d in Proc. ICASSP, 2016."], "authors": "Fabian-Robert St\u00f6ter, Bernd Edler", "email": "fabian-robert.stoeter@audiolabs-erlangen.de"}, {"short": "UHL1", "is_supervised": true, "description": "In this approach, we train for each instrument a (single-channel) ReLU network with K=4 layers from P = 12e6 training samples, which are pre-processed with a PCA. The training material comes from three different sources: an internal instrument loop catalogue, non-bleeding stems from MedleyDB and stems from SiSEC DEV. One training sample is created by mixing random audio from each instrument (with random amplitudes). Finally, the four FNN outputs are combined by a multi-channel Wiener filter. We estimate the PSDs and spatial covariance matrices for each instrument as described by Nugraha (\"weighted scheme\").\n", "uses_augmentation": true, "method_name": "Feed_Forward_NN", "affiliation": "Sony Corporation", "code": null, "references": ["S. Uhlich, F. Giron, and Y. Mitsufuji. \"Deep neural network based instrument extraction from music.\" ICASSP, 2015.", "A. A. Nugraha, A. Liutkus, and E. Vincent. \"Multichannel music separation with deep neural networks.\" EUSIPCO, 2016."], "authors": "Stefan Uhlich, Marcello Porcu, Franck Giron, Michael Enenkl, Thomas Kemp, Yuki Mitsufuji, Naoya Takahashi", "email": "stefan.uhlich@eu.sony.com, yuhki.mitsufuji@jp.sony.com"}, {"short": "UHL2", "is_supervised": true, "description": "In our second approach, we use a (stereo) recurrent neural network architecture with bi-directional LSTM (BLSTM) layers. It is trained solemnly on SiSEC DEV and uses data augmentation to avoid overfitting problems. In particular, we use the following augmentations (done on-the-fly to prepare one mini-batch): - random swapping left/right channel for each instrument, - random scaling with amplitudes from [0.25,1.25], - random chunking into sequences for each instrument, and, - random combination of instruments from different mixtures Finally, the four LSTM outputs are combined by a multi-channel Wiener filter. We estimate the PSDs and spatial covariance matrices for each instrument as described by Nugraha (\"weighted scheme\").\n", "uses_augmentation": true, "method_name": "LSTM_NN", "affiliation": "Sony Corporation", "code": null, "references": ["S. Uhlich, M. Porcu, F. Giron, M. Enenkl, T. Kemp, Y. Mitsufuji, N. Takahashi, Combining DNNs for Enhanced Music Source Separation, to appear", "A. A. Nugraha, A. Liutkus, and E. Vincent. \"Multichannel music separation with deep neural networks.\" EUSIPCO, 2016."], "authors": "Stefan Uhlich, Marcello Porcu, Franck Giron, Michael Enenkl, Thomas Kemp, Yuki Mitsufuji, Naoya Takahashi", "email": "stefan.uhlich@eu.sony.com, yuhki.mitsufuji@jp.sony.com"}, {"code": null, "is_supervised": true, "description": "This system is doing a linear combination of the systems \"UHL1\" and \"UHL2\" as follows:\n\\hat S_i,UHL3(m,k) = \\lambda * \\hat S_i,UHL1(m,k) + (1-\\lambda) * \\hat S_i,UHL2(m,k)\nfor all i = {\"Bass\", \"Drums\", \"Other\", \"Vocals\"} where \\hat S_i,UHL1(m,k) is the raw output of the feed-forward network and \\hat S_i,UHL2(m,k) is the raw output of the LSTM network. After the combination, we compute a multi-channel Wiener filter to reduce interference and artefacts (cf. Nugraha \"weighted scheme\").\n", "uses_augmentation": true, "method_name": "COMBO_FNN_LSTM", "affiliation": "Sony Corporation", "short": "UHL3", "references": ["S. Uhlich, M. Porcu, F. Giron, M. Enenkl, T. Kemp, Y. Mitsufuji, N. Takahashi, Combining DNNs for Enhanced Music Source Separation, to appear", "A. A. Nugraha, A. Liutkus, and E. Vincent. \"Multichannel music separation with deep neural networks.\" EUSIPCO, 2016."], "authors": "Stefan Uhlich, Marcello Porcu, Franck Giron, Michael Enenkl, Thomas Kemp, Yuki Mitsufuji, Naoya Takahashi", "email": "stefan.uhlich@eu.sony.com, yuhki.mitsufuji@jp.sony.com"}]
